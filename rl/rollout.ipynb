{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from kaggle_environments import evaluate, make\n",
    "\n",
    "import random\n",
    "from random import choice\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectX(gym.Env):\n",
    "    def __init__(self, switch_prob=0.5):\n",
    "        self.env = make('connectx', debug=True)\n",
    "        self.pair = [None, 'random']\n",
    "        self.trainer = self.env.train(self.pair)\n",
    "        self.switch_prob = switch_prob\n",
    "\n",
    "        # Define required gym fields (examples):\n",
    "        config = self.env.configuration\n",
    "        self.action_space = gym.spaces.Discrete(config.columns)\n",
    "        self.observation_space = gym.spaces.Discrete(\n",
    "            config.columns * config.rows)\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.trainer.step(action)\n",
    "\n",
    "    def set_state(self, init_state):\n",
    "        self.env.reset()\n",
    "        self.env.state[0]['observation'] = deepcopy(init_state)\n",
    "\n",
    "    def reset(self):\n",
    "        return self.trainer.reset()\n",
    "\n",
    "    def render(self, **kwargs):\n",
    "        return self.env.render(**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ConnectX()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarlo:\n",
    "    def __init__(self, env, gamma, episodes):\n",
    "        '''\n",
    "        env是当前需要进行模拟采样的状态\n",
    "        gamma用于计算Reward\n",
    "        episodes是每个(s,a)的采样次数\n",
    "        '''\n",
    "        self.state = deepcopy(env)\n",
    "        self.all_rewards = [0, 0, 0, 0, 0, 0, 0]\n",
    "        self.gamma = gamma\n",
    "        self.episodes = episodes\n",
    "\n",
    "    def rollout(self, env):\n",
    "        # 对当前状态进行rollout采样\n",
    "        mean_reward = 0\n",
    "        ini_state = env.env.state[0]['observation']\n",
    "        for i in range(self.episodes):\n",
    "            r = 0\n",
    "            gamma = 1\n",
    "            env.set_state(ini_state)\n",
    "            state = deepcopy(ini_state)\n",
    "\n",
    "            #env.env = deepcopy(ini_state)\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                space_list = [c for c in range(7) if state['board'][c] == 0]\n",
    "                action = choice(space_list)\n",
    "\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "\n",
    "                if done:\n",
    "                    if reward == 1:\n",
    "                        r += 20*gamma\n",
    "                    elif reward == 0:\n",
    "                        r -= 20*gamma\n",
    "                    else:\n",
    "                        r += 1*gamma\n",
    "\n",
    "                    mean_reward += r\n",
    "                else:\n",
    "                    r -= 0.05*gamma\n",
    "\n",
    "                gamma *= self.gamma\n",
    "                state = next_state\n",
    "\n",
    "        return mean_reward/self.episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1\t\t\t\t    # discount factor γ\n",
    "episodes = 1                # 模拟采样轮数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(num,env,gamma,episodes):\n",
    "    result = {'win':0,'loss':0,'draw':0}\n",
    "    for i in tqdm(range(num)):\n",
    "        #print('[GAME{}]:'.format(i))\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        while not done:\n",
    "            ini_state = env.env.state[0]['observation']\n",
    "            space_list = [c for c in range(7) if ini_state['board'][c] == 0]\n",
    "            R = []\n",
    "            for action in range(7):  # rollout模拟采样\n",
    "                if action not in space_list:\n",
    "                    R.append(-1)\n",
    "                    continue\n",
    "                \n",
    "                env.set_state(ini_state)\n",
    "\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                mc = MonteCarlo(env, gamma, episodes)\n",
    "                reward = reward + gamma*mc.rollout(env)\n",
    "                R.append(reward)\n",
    "    \n",
    "            Action = int(np.argmax(R))  # 根据采样结果选择动作\n",
    "            \n",
    "\n",
    "            env.set_state(ini_state)\n",
    "            next_state, reward, done, info = env.step(Action)\n",
    "\n",
    "            #print('R = ', R)\n",
    "            #print('Action = ', Action)\n",
    "            #print(env.render(mode=\"ansi\"))\n",
    "\n",
    "            if done:\n",
    "                if reward == 1:  # Won\n",
    "                    result['win'] += 1\n",
    "                    #print('you win!')\n",
    "                elif reward == 0:  # Lost\n",
    "                    result['loss'] += 1\n",
    "                    #print('you loss!')\n",
    "                else:\n",
    "                    result['draw'] += 1\n",
    "                    #print('draw!')\n",
    "    print('My Agent vs Random Agent:', '{0} episodes- won: {1} | loss: {2} | draw: {3} | winning rate: {4}%'.format(\n",
    "        num,\n",
    "        result['win'],\n",
    "        result['loss'],\n",
    "        result['draw'],\n",
    "        (result['win'] / num)*100\n",
    "    ))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b62ee8e0c8a84cd48e34b4b9126c5431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "play(100,env,gamma = 1,episodes = 1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07efdcd4b820c98a756949507a4d29d7862823915ec7477944641bea022f4f62"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
